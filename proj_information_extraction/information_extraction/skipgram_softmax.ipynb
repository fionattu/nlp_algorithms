{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skipgram_softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6lMKKc8bPbyVQTPQWtUlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fionattu/nlp_algorithms/blob/master/proj_information_extraction/information_extraction/skipgram_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ER8kqZZawo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "cf12e089-93ec-41a1-849d-4626d2c85275"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "sentences = [ \"i like dog\", \"i like cat\", \"i like animal\",\n",
        "              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n",
        "              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n",
        "              \"apple i movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
        "\n",
        "word_sequence = \" \".join(sentences).split()\n",
        "word_list = list(set(word_sequence))\n",
        "word_dict = {w : i for i, w in enumerate(word_list)}\n",
        "print(word_dict)\n",
        "\n",
        "# Creating n-grams with a sliding window, n=3\n",
        "skip_grams = []\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "  center_w = word_dict[word_sequence[i]]\n",
        "  context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "  for context_w in context:\n",
        "    skip_grams.append([center_w, context_w])\n",
        "\n",
        "\n",
        "def random_batch(n_grams, batch_size):\n",
        "  batch_inputs = []\n",
        "  batch_labels = []\n",
        "  random_indices = np.random.choice(range(len(n_grams)), batch_size, replace=False)\n",
        "\n",
        "  for i in random_indices:\n",
        "    batch_inputs.append(np.eye(voc_size)[n_grams[i][0]])  # one-hot of center_w\n",
        "    batch_labels.append(n_grams[i][1])  # index of context_w\n",
        "\n",
        "  return batch_inputs, batch_labels\n",
        "\n",
        "# Word2Vec Parameter\n",
        "batch_size = 20  # To show 2 dim embedding graph\n",
        "embedding_size = 2  # To show 2 dim embedding graph\n",
        "voc_size = len(word_list)\n",
        "\n",
        "# Model\n",
        "class Word2Vec(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    \n",
        "    # W for hidden layer, WT for output layer, not transpose relation\n",
        "    self.W = nn.Parameter(-2 * torch.rand(size=(voc_size, embedding_size)) + 1).type(torch.FloatTensor)\n",
        "    self.WT = nn.Parameter(-2 * torch.rand(size=(embedding_size, voc_size)) + 1).type(torch.FloatTensor)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X : [batch_size, voc_size]\n",
        "    hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
        "    output_layer = torch.matmul(hidden_layer, self.WT) # output_layer : [batch_size, voc_size]\n",
        "    return output_layer\n",
        "  \n",
        "model = Word2Vec()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(5000):\n",
        "    batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
        "\n",
        "    batch_inputs = Variable(torch.Tensor(batch_inputs))\n",
        "    batch_labels = Variable(torch.LongTensor(batch_labels))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch_inputs)\n",
        "\n",
        "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
        "    loss = criterion(output, batch_labels)\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hate': 0, 'i': 1, 'cat': 2, 'music': 3, 'dog': 4, 'eyes': 5, 'animal': 6, 'like': 7, 'movie': 8, 'fish': 9, 'book': 10, 'apple': 11, 'milk': 12}\n",
            "Epoch: 1000 cost = 2.298052\n",
            "Epoch: 2000 cost = 2.090854\n",
            "Epoch: 3000 cost = 1.775612\n",
            "Epoch: 4000 cost = 1.544817\n",
            "Epoch: 5000 cost = 1.608474\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}