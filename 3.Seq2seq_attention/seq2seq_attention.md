## Neural Machine Translation with Seq2Seq

### Sequence-to-sequence Basics
### Sequence-to-sequence Architecture - encoder
* stacked LSTM in reverse order (how about the dimension)
### Sequence-to-sequence Architecture - decoder
 * bean search
 * softmax 
 * start and end of decoder

 bidirectional stacked LSTM?

## Attention Mechanism

### basics

how attention work in NMT
can deal with long sentence

## Evaluation of Machine Translation Systems
